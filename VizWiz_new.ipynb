{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clone this repo and then move the notebook to that folder and run it\n",
    "\n",
    "!git clone https://github.com/d-li14/mobilenetv3.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mobilenetv3 import mobilenetv3_large,mobilenetv3_small\n",
    "import h5py\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import re\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from itertools import takewhile\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "class ImageDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load the paths to the images available in the folder\n",
    "        self.image_names = self._load_img_paths()\n",
    "\n",
    "        if len(self.image_names) == 0:\n",
    "            raise (RuntimeError(\"Found 0 images in \" + path + \"\\n\"\n",
    "                                                              \"Supported image extensions are: \" + \",\".join(\n",
    "                IMG_EXTENSIONS)))\n",
    "        else:\n",
    "            print('Found {} images in {}'.format(len(self), self.path))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = {}\n",
    "        item['name'] = self.image_names[index]\n",
    "        item['path'] = os.path.join(self.path, item['name'])\n",
    "\n",
    "        # Use PIL to load the image\n",
    "        item['visual'] = Image.open(item['path']).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            item['visual'] = self.transform(item['visual'])\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def _load_img_paths(self):\n",
    "        images = []\n",
    "        for name in os.listdir(self.path):\n",
    "            if is_image_file(name):\n",
    "                images.append(name)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(img_size):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        # TODO : Compute mean and std of VizWiz\n",
    "        # ImageNet normalization\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetFeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NetFeatureExtractor, self).__init__()\n",
    "        self.model = mobilenetv3_large()\n",
    "        self.model.classifier[0] = nn.Sequential()\n",
    "        self.model.classifier[1] = nn.Linear(960,1280)\n",
    "        self.model.classifier[3] = nn.Sequential()\n",
    "        self.model.classifier[5] = nn.Linear(1280,1000)\n",
    "        # Input of size 224x224x3.\n",
    "        self.model.load_state_dict(torch.load('pretrained/mobilenetv3-large-657e7b3d.pth'))\n",
    "\n",
    "        # Save attention features (tensor)\n",
    "        def save_att_features(module, input, output):\n",
    "            self.att_feat = output\n",
    "\n",
    "        # Save no-attention features (vector)\n",
    "        def save_noatt_features(module, input, output):\n",
    "            self.no_att_feat = output\n",
    "\n",
    "        # This is a forward hook. Is executed each time forward is executed\n",
    "        self.model.conv.register_forward_hook(save_att_features)\n",
    "        self.model.avgpool.register_forward_hook(save_noatt_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.model(x)\n",
    "        return self.no_att_feat, self.att_feat  # [batch_size, 960], [batch_size, 960, 7, 7]\n",
    "\n",
    "\n",
    "def image_feature_extractor_train():\n",
    "    # Benchmark mode is good whenever your input sizes for your network do not vary\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    net = NetFeatureExtractor().cuda()\n",
    "    net.eval()\n",
    "    # Resize, Crop, Normalize\n",
    "    transform = get_transform(224)\n",
    "    dataset = ImageDataset(\"train\", transform=transform)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,batch_size=4,num_workers=4,shuffle=False,pin_memory=True)\n",
    "\n",
    "    h5_file = h5py.File(\"mobile_net_train.h5\", 'w')\n",
    "\n",
    "    dummy_input = Variable(torch.ones(1, 3, 224, 224), volatile=True).cuda()\n",
    "    _, dummy_output = net(dummy_input)\n",
    "\n",
    "    att_features_shape = (len(data_loader.dataset),dummy_output.size(1),dummy_output.size(2),dummy_output.size(3))\n",
    "\n",
    "    noatt_features_shape = (len(data_loader.dataset),dummy_output.size(1))\n",
    "\n",
    "    h5_att = h5_file.create_dataset('att', shape=att_features_shape, dtype='float16')\n",
    "    h5_noatt = h5_file.create_dataset('noatt', shape=noatt_features_shape, dtype='float16')\n",
    "\n",
    "    # save order of extraction\n",
    "    dt = h5py.special_dtype(vlen=str)\n",
    "    img_names = h5_file.create_dataset('img_name', shape=(len(data_loader.dataset),), dtype=dt)\n",
    "\n",
    "    begin = time.time()\n",
    "    end = time.time()\n",
    "\n",
    "    print('Extracting features ...')\n",
    "    idx = 0\n",
    "    delta = 4\n",
    "\n",
    "    for i, inputs in enumerate(tqdm(data_loader)):\n",
    "        with torch.no_grad():\n",
    "            inputs_img = Variable(inputs['visual']).cuda()\n",
    "            no_att_feat, att_feat = net(inputs_img)\n",
    "\n",
    "            # reshape (batch_size, 960)\n",
    "            no_att_feat = no_att_feat.view(-1, 960)\n",
    "\n",
    "            h5_noatt[idx:idx + delta] = no_att_feat.data.cpu().numpy().astype('float16')\n",
    "            h5_att[idx:idx + delta, :, :] = att_feat.data.cpu().numpy().astype('float16')\n",
    "            img_names[idx:idx + delta] = inputs['name']\n",
    "\n",
    "            idx += delta\n",
    "    h5_file.close()\n",
    "\n",
    "    end = time.time() - begin\n",
    "\n",
    "    print('Finished in {}m and {}s'.format(int(end / 60), int(end % 60)))\n",
    "    print('Created file ')\n",
    "    \n",
    "def image_feature_extractor_val():\n",
    "    # Benchmark mode is good whenever your input sizes for your network do not vary\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    net = NetFeatureExtractor().cuda()\n",
    "    net.eval()\n",
    "    # Resize, Crop, Normalize\n",
    "    transform = get_transform(224)\n",
    "    dataset = ImageDataset(\"val\", transform=transform)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,batch_size=4,num_workers=4,shuffle=False,pin_memory=True)\n",
    "\n",
    "    h5_file = h5py.File(\"mobile_net_val.h5\", 'w')\n",
    "\n",
    "    dummy_input = Variable(torch.ones(1, 3, 224, 224), volatile=True).cuda()\n",
    "    _, dummy_output = net(dummy_input)\n",
    "\n",
    "    att_features_shape = (len(data_loader.dataset),dummy_output.size(1),dummy_output.size(2),dummy_output.size(3))\n",
    "\n",
    "    noatt_features_shape = (len(data_loader.dataset),dummy_output.size(1))\n",
    "\n",
    "    h5_att = h5_file.create_dataset('att', shape=att_features_shape, dtype='float16')\n",
    "    h5_noatt = h5_file.create_dataset('noatt', shape=noatt_features_shape, dtype='float16')\n",
    "\n",
    "    # save order of extraction\n",
    "    dt = h5py.special_dtype(vlen=str)\n",
    "    img_names = h5_file.create_dataset('img_name', shape=(len(data_loader.dataset),), dtype=dt)\n",
    "\n",
    "    begin = time.time()\n",
    "    end = time.time()\n",
    "\n",
    "    print('Extracting features ...')\n",
    "    idx = 0\n",
    "    delta = 4\n",
    "\n",
    "    for i, inputs in enumerate(tqdm(data_loader)):\n",
    "        with torch.no_grad():\n",
    "            inputs_img = Variable(inputs['visual']).cuda()\n",
    "            no_att_feat, att_feat = net(inputs_img)\n",
    "\n",
    "            # reshape (batch_size, 960)\n",
    "            no_att_feat = no_att_feat.view(-1, 960)\n",
    "\n",
    "            h5_noatt[idx:idx + delta] = no_att_feat.data.cpu().numpy().astype('float16')\n",
    "            h5_att[idx:idx + delta, :, :] = att_feat.data.cpu().numpy().astype('float16')\n",
    "            img_names[idx:idx + delta] = inputs['name']\n",
    "\n",
    "            idx += delta\n",
    "    h5_file.close()\n",
    "\n",
    "    end = time.time() - begin\n",
    "\n",
    "    print('Finished in {}m and {}s'.format(int(end / 60), int(end % 60)))\n",
    "    print('Created file : ')\n",
    "    \n",
    "def image_feature_extractor_test():\n",
    "    # Benchmark mode is good whenever your input sizes for your network do not vary\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    net = NetFeatureExtractor().cuda()\n",
    "    net.eval()\n",
    "    # Resize, Crop, Normalize\n",
    "    transform = get_transform(224)\n",
    "    dataset = ImageDataset(\"test\", transform=transform)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,batch_size=4,num_workers=4,shuffle=False,pin_memory=True)\n",
    "\n",
    "    h5_file = h5py.File(\"mobile_net_test.h5\", 'w')\n",
    "\n",
    "    dummy_input = Variable(torch.ones(1, 3, 224, 224), volatile=True).cuda()\n",
    "    _, dummy_output = net(dummy_input)\n",
    "\n",
    "    att_features_shape = (len(data_loader.dataset),dummy_output.size(1),dummy_output.size(2),dummy_output.size(3))\n",
    "\n",
    "    noatt_features_shape = (len(data_loader.dataset),dummy_output.size(1))\n",
    "\n",
    "    h5_att = h5_file.create_dataset('att', shape=att_features_shape, dtype='float16')\n",
    "    h5_noatt = h5_file.create_dataset('noatt', shape=noatt_features_shape, dtype='float16')\n",
    "\n",
    "    # save order of extraction\n",
    "    dt = h5py.special_dtype(vlen=str)\n",
    "    img_names = h5_file.create_dataset('img_name', shape=(len(data_loader.dataset),), dtype=dt)\n",
    "\n",
    "    begin = time.time()\n",
    "    end = time.time()\n",
    "\n",
    "    print('Extracting features ...')\n",
    "    idx = 0\n",
    "    delta = 4\n",
    "\n",
    "    for i, inputs in enumerate(tqdm(data_loader)):\n",
    "        with torch.no_grad():\n",
    "            inputs_img = Variable(inputs['visual']).cuda()\n",
    "            no_att_feat, att_feat = net(inputs_img)\n",
    "\n",
    "            # reshape (batch_size, 960)\n",
    "            no_att_feat = no_att_feat.view(-1, 960)\n",
    "\n",
    "            h5_noatt[idx:idx + delta] = no_att_feat.data.cpu().numpy().astype('float16')\n",
    "            h5_att[idx:idx + delta, :, :] = att_feat.data.cpu().numpy().astype('float16')\n",
    "            img_names[idx:idx + delta] = inputs['name']\n",
    "\n",
    "            idx += delta\n",
    "    h5_file.close()\n",
    "\n",
    "    end = time.time() - begin\n",
    "\n",
    "    print('Finished in {}m and {}s'.format(int(end / 60), int(end % 60)))\n",
    "    print('Created file : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_questions(annotations):\n",
    "    \"\"\" Filter, Normalize and Tokenize question. \"\"\"\n",
    "\n",
    "    prepared = []\n",
    "    questions = [q['question'] for q in annotations]\n",
    "\n",
    "    for question in questions:\n",
    "        # lower case\n",
    "        question = question.lower()\n",
    "\n",
    "        # define desired replacements here\n",
    "        punctuation_dict = {'.': ' ', \"'\": '', '?': ' ', '_': ' ', '-': ' ', '/': ' ', ',': ' '}\n",
    "        conversational_dict = {\"thank you\": '', \"thanks\": '', \"thank\": '', \"please\": '', \"hello\": '',\n",
    "                               \"hi \": ' ', \"hey \": ' ', \"good morning\": '', \"good afternoon\": '', \"have a nice day\": '',\n",
    "                               \"okay\": '', \"goodbye\": ''}\n",
    "\n",
    "        rep = punctuation_dict\n",
    "        rep.update(conversational_dict)\n",
    "\n",
    "        # use these three lines to do the replacement\n",
    "        rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "        pattern = re.compile(\"|\".join(rep.keys()))\n",
    "        question = pattern.sub(lambda m: rep[re.escape(m.group(0))], question)\n",
    "\n",
    "        # sentence to list\n",
    "        question = question.split(' ')\n",
    "\n",
    "        # remove empty strings\n",
    "        question = list(filter(None, question))\n",
    "\n",
    "        prepared.append(question)\n",
    "\n",
    "    return prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_answers(annotations):\n",
    "    answers = [[a['answer'] for a in ans_dict['answers']] for ans_dict in annotations]\n",
    "    prepared = []\n",
    "\n",
    "    for sample_answers in answers:\n",
    "        prepared_sample_answers = []\n",
    "        for answer in sample_answers:\n",
    "            # lower case\n",
    "            answer = answer.lower()\n",
    "\n",
    "            # define desired replacements here\n",
    "            punctuation_dict = {'.': ' ', \"'\": '', '?': ' ', '_': ' ', '-': ' ', '/': ' ', ',': ' '}\n",
    "\n",
    "            rep = punctuation_dict\n",
    "            rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "            pattern = re.compile(\"|\".join(rep.keys()))\n",
    "            answer = pattern.sub(lambda m: rep[re.escape(m.group(0))], answer)\n",
    "            prepared_sample_answers.append(answer)\n",
    "\n",
    "        prepared.append(prepared_sample_answers)\n",
    "    return prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_vocab(questions, min_count=0):\n",
    "    \"\"\"\n",
    "    Extract vocabulary used to tokenize and encode questions.\n",
    "    \"\"\"\n",
    "    words = itertools.chain.from_iterable([q for q in questions])  # chain('ABC', 'DEF') --> A B C D E F\n",
    "    counter = Counter(words)\n",
    "\n",
    "    counted_words = counter.most_common()\n",
    "    # select only the words appearing at least min_count\n",
    "    selected_words = list(takewhile(lambda x: x[1] >= min_count, counted_words))\n",
    "\n",
    "    vocab = {t[0]: i for i, t in enumerate(selected_words, start=1)}\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def create_answer_vocab(annotations, top_k):\n",
    "    answers = itertools.chain.from_iterable(prepare_answers(annotations))\n",
    "\n",
    "    counter = Counter(answers)\n",
    "    counted_ans = counter.most_common(top_k)\n",
    "    # start from labels from 0\n",
    "    vocab = {t[0]: i for i, t in enumerate(counted_ans, start=0)}\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer_vocabulary(mode = \"train\"):\n",
    "    # Load annotations\n",
    "    dir_path = \"Annotations\"\n",
    "\n",
    "    # vocabs are created based on train (trainval) split only\n",
    "    train_path = os.path.join(dir_path, mode + '.json')\n",
    "    with open(train_path, 'r',encoding = 'utf-8') as fd:\n",
    "        train_ann = json.load(fd)\n",
    "\n",
    "    questions = prepare_questions(train_ann)\n",
    "\n",
    "    question_vocab = create_question_vocab(questions, 0)\n",
    "    answer_vocab = create_answer_vocab(train_ann, 3000)\n",
    "\n",
    "    # Save pre-processing vocabs\n",
    "    vocabs = {\n",
    "        'question': question_vocab,\n",
    "        'answer': answer_vocab,\n",
    "    }\n",
    "\n",
    "    with open(\"vocabs.json\", 'w') as fd:\n",
    "        json.dump(vocabs, fd)\n",
    "\n",
    "    print(\"vocabs saved in {}\".format(\"vocabs.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(question, token_to_index, max_length):\n",
    "    question_vec = torch.zeros(max_length).long()\n",
    "    length = min(len(question), max_length)\n",
    "    for i in range(length):\n",
    "        token = question[i]\n",
    "        index = token_to_index.get(token, 0)\n",
    "        question_vec[i] = index\n",
    "    # empty encoded questions are a problem when packed,\n",
    "    # if we set min length 1 we feed a 0 token to the RNN\n",
    "    # that is not a problem since the token 0 does not represent a word\n",
    "    return question_vec, max(length, 1)\n",
    "\n",
    "\n",
    "def encode_answers(answers, answer_to_index):\n",
    "    answer_vec = torch.zeros(len(answer_to_index))\n",
    "    for answer in answers:\n",
    "        index = answer_to_index.get(answer)\n",
    "        if index is not None:\n",
    "            answer_vec[index] += 1\n",
    "    return answer_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, features_path, mode):\n",
    "        self.path_hdf5 = features_path\n",
    "\n",
    "        assert os.path.isfile(self.path_hdf5), \\\n",
    "            'File not found in {}, you must extract the features first with images_preprocessing.py'.format(\n",
    "                self.path_hdf5)\n",
    "\n",
    "        self.hdf5_file = h5py.File(self.path_hdf5, 'r')\n",
    "        self.dataset_features = self.hdf5_file[mode]  # noatt or att (attention)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(self.dataset_features[index].astype('float32'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(data.Dataset):\n",
    "    \"\"\" VQA dataset, open-ended \"\"\"\n",
    "\n",
    "    def __init__(self, split,h5_path):\n",
    "        super(VQADataset, self).__init__()\n",
    "\n",
    "        with open(\"vocabs.json\", 'r',encoding = 'utf-8') as fd:\n",
    "            vocabs = json.load(fd)\n",
    "\n",
    "        annotations_dir = \"Annotations\"\n",
    "\n",
    "        path_ann = os.path.join(annotations_dir, split + \".json\")\n",
    "        with open(path_ann, 'r',encoding = 'utf-8') as fd:\n",
    "            self.annotations = json.load(fd)\n",
    "\n",
    "        self.max_question_length = 26\n",
    "        self.split = split\n",
    "\n",
    "        # vocab\n",
    "        self.vocabs = vocabs\n",
    "        self.token_to_index = self.vocabs['question']\n",
    "        self.answer_to_index = self.vocabs['answer']\n",
    "\n",
    "        # pre-process questions and answers\n",
    "        self.questions = prepare_questions(self.annotations)\n",
    "        self.questions = [encode_question(q, self.token_to_index, self.max_question_length) for q in\n",
    "                          self.questions]  # encode questions and return question and question lenght\n",
    "\n",
    "        if self.split != 'test':\n",
    "            self.answers = prepare_answers(self.annotations)\n",
    "            self.answers = [encode_answers(a, self.answer_to_index) for a in\n",
    "                            self.answers]  # create a sparse vector of len(self.answer_to_index) for each question containing the occurances of each answer\n",
    "\n",
    "        if self.split == \"train\" or self.split == \"trainval\":\n",
    "            self._filter_unanswerable_samples()\n",
    "\n",
    "        # load image names in feature extraction order\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            img_names = f['img_name'][()]\n",
    "        self.name_to_id = {name: i for i, name in enumerate(img_names)}\n",
    "\n",
    "        # names in the annotations, will be used to get items from the dataset\n",
    "        self.img_names = [s['image'] for s in self.annotations]\n",
    "        # load features\n",
    "        self.features = FeaturesDataset(h5_path, \"att\")\n",
    "\n",
    "    def _filter_unanswerable_samples(self):\n",
    "        \"\"\"\n",
    "        Filter during training the samples that do not have at least one answer\n",
    "        \"\"\"\n",
    "        a = []\n",
    "        q = []\n",
    "        annotations = []\n",
    "        for i in range(len(self.answers)):\n",
    "            if len(self.answers[i].nonzero()) > 0:\n",
    "                a.append(self.answers[i])\n",
    "                q.append(self.questions[i])\n",
    "\n",
    "                annotations.append(self.annotations[i])\n",
    "        self.answers = a\n",
    "        self.questions = q\n",
    "        self.annotations = annotations\n",
    "\n",
    "    @property\n",
    "    def num_tokens(self):\n",
    "        return len(self.token_to_index) + 1  # add 1 for <unknown> token at index 0\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        item = {}\n",
    "        item['question'], item['q_length'] = self.questions[i]\n",
    "        if self.split != 'test':\n",
    "            item['answer'] = self.answers[i]\n",
    "        img_name = self.img_names[i]\n",
    "        feature_id = self.name_to_id[img_name]\n",
    "        item['img_name'] = self.img_names[i]\n",
    "        item['visual'] = self.features[feature_id]\n",
    "        # collate_fn sorts the samples in order to be possible to pack them later in the model.\n",
    "        # the sample_id is returned so that the original order can be restored during when evaluating the predictions\n",
    "        item['sample_id'] = i\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Sort samples in the batch based on the question lengths in descending order.\n",
    "    # This allows to pack the pack_padded_sequence when encoding questions using RNN\n",
    "    batch.sort(key=lambda x: x['q_length'], reverse=True)\n",
    "    return data.dataloader.default_collate(batch)\n",
    "\n",
    "def get_loader(split,h5_path):\n",
    "    \"\"\" Returns the data loader of the specified dataset split \"\"\"\n",
    "    dataset = VQADataset(split,h5_path)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True if split == 'train' or split == 'trainval' else False,  # only shuffle the data in training\n",
    "        pin_memory=True,\n",
    "        num_workers=4,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    References :\n",
    "     1 - https://arxiv.org/abs/1704.03162\n",
    "     2 - https://arxiv.org/pdf/1511.02274\n",
    "     3 - https://arxiv.org/abs/1708.00584\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_tokens):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        dim_v = 960\n",
    "        dim_q = 1024\n",
    "        dim_h = 1024\n",
    "\n",
    "        n_glimpses = 2\n",
    "\n",
    "        self.text = TextEncoder(\n",
    "            num_tokens=num_tokens,\n",
    "            emb_size=300,\n",
    "            dim_q=dim_q,\n",
    "            drop=0.25,\n",
    "        )\n",
    "        self.attention = Attention(\n",
    "            dim_v=dim_v,\n",
    "            dim_q=dim_q,\n",
    "            dim_h=512,\n",
    "            n_glimpses=n_glimpses,\n",
    "            drop=0.5,\n",
    "        )\n",
    "        self.classifier = Classifier(\n",
    "            dim_input=n_glimpses * dim_v + dim_q,\n",
    "            dim_h=dim_h,\n",
    "            top_ans=3000,\n",
    "            drop=0.5,\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, v, q, q_len):\n",
    "\n",
    "        q = self.text(q, list(q_len.data))\n",
    "        # L2 normalization on the depth dimension\n",
    "        v = F.normalize(v, p=2, dim=1)\n",
    "        attention_maps = self.attention(v, q)\n",
    "        v = apply_attention(v, attention_maps)\n",
    "        # concatenate attended features and encoded question\n",
    "        combined = torch.cat([v, q], dim=1)\n",
    "        answer = self.classifier(combined)\n",
    "        return answer\n",
    "\n",
    "\n",
    "class Classifier(nn.Sequential):\n",
    "    def __init__(self, dim_input, dim_h, top_ans, drop=0.0):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.add_module('drop1', nn.Dropout(drop))\n",
    "        self.add_module('lin1', nn.Linear(dim_input, dim_h))\n",
    "        self.add_module('relu', nn.ReLU())\n",
    "        self.add_module('drop2', nn.Dropout(drop))\n",
    "        self.add_module('lin2', nn.Linear(dim_h, top_ans))\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, num_tokens, emb_size, dim_q, drop=0.0):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_tokens, emb_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(input_size=emb_size,\n",
    "                            hidden_size=dim_q,\n",
    "                            num_layers=1)\n",
    "        self.dim_q = dim_q\n",
    "\n",
    "        # Initialize parameters\n",
    "        self._init_lstm(self.lstm.weight_ih_l0)\n",
    "        self._init_lstm(self.lstm.weight_hh_l0)\n",
    "        self.lstm.bias_ih_l0.data.zero_()\n",
    "        self.lstm.bias_hh_l0.data.zero_()\n",
    "\n",
    "        init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def _init_lstm(self, weight):\n",
    "        for w in weight.chunk(4, 0):\n",
    "            init.xavier_uniform_(w)\n",
    "\n",
    "    def forward(self, q, q_len):\n",
    "        embedded = self.embedding(q)\n",
    "        tanhed = self.tanh(self.dropout(embedded))\n",
    "        # pack to feed to the LSTM\n",
    "        packed = pack_padded_sequence(tanhed, q_len, batch_first=True)\n",
    "        _, (h, _) = self.lstm(packed)\n",
    "        # _, (_, c) = self.lstm(packed)\n",
    "        return h.squeeze(0)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_v, dim_q, dim_h, n_glimpses, drop=0.0):\n",
    "        super(Attention, self).__init__()\n",
    "        # As specified in https://arxiv.org/pdf/1511.02274.pdf the bias is already included in fc_q\n",
    "        self.conv_v = nn.Conv2d(dim_v, dim_h, 1, bias=False)\n",
    "        self.fc_q = nn.Linear(dim_q, dim_h)\n",
    "        self.conv_x = nn.Conv2d(dim_h, n_glimpses, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, v, q):\n",
    "        # bring to the same shape\n",
    "        v = self.conv_v(self.dropout(v))\n",
    "        q = self.fc_q(self.dropout(q))\n",
    "        q = repeat_encoded_question(q, v)\n",
    "        # sum element-wise and ReLU\n",
    "        x = self.relu(v + q)\n",
    "\n",
    "        x = self.conv_x(self.dropout(x))  # We obtain n_glimpses attention maps [batch_size][n_glimpses][14][14]\n",
    "        return x\n",
    "\n",
    "\n",
    "def repeat_encoded_question(q, v):\n",
    "    \"\"\"\n",
    "    Repeat the encoded question over all the spatial positions of the input image feature tensor.\n",
    "    :param q: shape [batch_size][h]\n",
    "    :param v: shape [batch_size][h][7][7]\n",
    "    :return: a tensor constructed repeating q 7x7 with shape [batch_size][h][7][7]\n",
    "    \"\"\"\n",
    "    batch_size, h = q.size()\n",
    "    # repeat the encoded question [14x14] times (over all the spatial positions of the image feature matrix)\n",
    "    q_tensor = q.view(batch_size, h, *([1, 1])).expand_as(v)\n",
    "    return q_tensor\n",
    "\n",
    "\n",
    "def apply_attention(v, attention):\n",
    "    \"\"\"\n",
    "    Apply attention maps over the input image features.\n",
    "    \"\"\"\n",
    "    batch_size, spatial_vec_size = v.size()[:2]\n",
    "    glimpses = attention.size(1)\n",
    "\n",
    "    # flatten the spatial dimensions [7x7] into a third dimension [49]\n",
    "    v = v.view(batch_size, spatial_vec_size, -1)\n",
    "    attention = attention.view(batch_size, glimpses, -1)\n",
    "    n_image_regions = v.size(2)  # 7x7 = 49\n",
    "\n",
    "    # Apply softmax to each attention map separately to create n_glimpses attention distribution over the image regions\n",
    "    attention = attention.view(batch_size * glimpses, -1)  # [batch_size x n_glimpses][49]\n",
    "    attention = F.softmax(attention, dim=1)\n",
    "\n",
    "    # apply the weighting by creating a new dim to tile both tensors over\n",
    "    target_size = [batch_size, glimpses, spatial_vec_size, n_image_regions]\n",
    "    v = v.view(batch_size, 1, spatial_vec_size, n_image_regions).expand(\n",
    "        *target_size)  # [batch_size][n_glimpses][960[49]\n",
    "    attention = attention.view(batch_size, glimpses, 1, n_image_regions).expand(\n",
    "        *target_size)  # [batch_size][n_glimpses][960][49]\n",
    "    # Weighted sum over all the spatial regions vectors\n",
    "    weighted = v * attention\n",
    "    weighted_mean = weighted.sum(dim=3)  # [batch_size][n_glimpses][960]\n",
    "\n",
    "    # attended features are flattened in the same dimension\n",
    "    return weighted_mean.view(batch_size, -1)  # [batch_size][n_glimpses * 960]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_accuracy(predicted, true):\n",
    "    \"\"\" Approximation of VQA accuracy metric \"\"\"\n",
    "    _, predicted_index = predicted.max(dim=1, keepdim=True)\n",
    "    agreeing = true.gather(dim=1, index=predicted_index)\n",
    "    return (agreeing * 0.33333).clamp(max=1)  # * 0.33333 is a good approximation of the VQA metric\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def track(self, name, *monitors):\n",
    "        l = Tracker.ListStorage(monitors)\n",
    "        self.data.setdefault(name, []).append(l)\n",
    "        return l\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {k: list(map(list, v)) for k, v in self.data.items()}\n",
    "\n",
    "    class ListStorage:\n",
    "        def __init__(self, monitors=[]):\n",
    "            self.data = []\n",
    "            self.monitors = monitors\n",
    "            for monitor in self.monitors:\n",
    "                setattr(self, monitor.name, monitor)\n",
    "\n",
    "        def append(self, item):\n",
    "            for monitor in self.monitors:\n",
    "                monitor.update(item)\n",
    "            self.data.append(item)\n",
    "\n",
    "        def __iter__(self):\n",
    "            return iter(self.data)\n",
    "\n",
    "    class MeanMonitor:\n",
    "        name = 'mean'\n",
    "\n",
    "        def __init__(self):\n",
    "            self.n = 0\n",
    "            self.total = 0\n",
    "\n",
    "        def update(self, value):\n",
    "            self.total += value\n",
    "            self.n += 1\n",
    "\n",
    "        @property\n",
    "        def value(self):\n",
    "            return self.total / self.n\n",
    "\n",
    "    class MovingMeanMonitor:\n",
    "        name = 'mean'\n",
    "\n",
    "        def __init__(self, momentum=0.9):\n",
    "            self.momentum = momentum\n",
    "            self.first = True\n",
    "            self.value = None\n",
    "\n",
    "        def update(self, value):\n",
    "            if self.first:\n",
    "                self.value = value\n",
    "                self.first = False\n",
    "            else:\n",
    "                m = self.momentum\n",
    "                self.value = m * self.value + (1 - m) * value\n",
    "\n",
    "\n",
    "def get_id_from_name(name):\n",
    "    import re\n",
    "\n",
    "    n = re.search('VizWiz_(.+?)_', name)\n",
    "    if n:\n",
    "        split = n.group(1)\n",
    "\n",
    "    m = re.search(('VizWiz_%s_(.+?).jpg' % split), name)\n",
    "    if m:\n",
    "        found = m.group(1)\n",
    "\n",
    "    return int(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23954 images in train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:44: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  0%|          | 0/5989 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5989/5989 [06:10<00:00, 16.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 6m and 10s\n",
      "Created file \n",
      "vocabs saved in vocabs.json\n"
     ]
    }
   ],
   "source": [
    "#Creating image features file\n",
    "image_feature_extractor_train()\n",
    "\n",
    "#Create question and answer vocabulary\n",
    "question_answer_vocabulary(mode = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabs saved in vocabs.json\n"
     ]
    }
   ],
   "source": [
    "question_answer_vocabulary(mode = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:99: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "\r",
      "  0%|          | 0/1938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7750 images in val\n",
      "Extracting features ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1938/1938 [01:52<00:00, 17.20it/s]\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:154: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 1m and 52s\n",
      "Created file : \n",
      "Found 8000 images in test\n",
      "Extracting features ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:04<00:00, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 2m and 4s\n",
      "Created file : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_feature_extractor_val()\n",
    "image_feature_extractor_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "def train(model, loader, optimizer, tracker, epoch, split):\n",
    "    model.train()\n",
    "\n",
    "    tracker_class, tracker_params = tracker.MovingMeanMonitor, {'momentum': 0.99}\n",
    "    tq = tqdm(loader, desc='{} E{:03d}'.format(split, epoch), ncols=0)\n",
    "    loss_tracker = tracker.track('{}_loss'.format(split), tracker_class(**tracker_params))\n",
    "    acc_tracker = tracker.track('{}_acc'.format(split), tracker_class(**tracker_params))\n",
    "    log_softmax = nn.LogSoftmax(dim=1).cuda()\n",
    "\n",
    "    for item in tq:\n",
    "        v = item['visual']\n",
    "        q = item['question']\n",
    "        a = item['answer']\n",
    "        q_length = item['q_length']\n",
    "\n",
    "        v = Variable(v.cuda())\n",
    "        q = Variable(q.cuda())\n",
    "        a = Variable(a.cuda())\n",
    "        q_length = Variable(q_length.cuda())\n",
    "\n",
    "        out = model(v, q, q_length)\n",
    "\n",
    "        # This is the Soft-loss described in https://arxiv.org/pdf/1708.00584.pdf\n",
    "\n",
    "        nll = -log_softmax(out)\n",
    "\n",
    "        loss = (nll * a / 10).sum(dim=1).mean()\n",
    "        acc = vqa_accuracy(out.data, a.data).cpu()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_tracker.append(loss.item())\n",
    "        acc_tracker.append(acc.mean())\n",
    "        fmt = '{:.4f}'.format\n",
    "        tq.set_postfix(loss=fmt(loss_tracker.mean.value), acc=fmt(acc_tracker.mean.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating\n",
    "def evaluate(model, loader, tracker, epoch, split):\n",
    "    model.eval()\n",
    "    tracker_class, tracker_params = tracker.MeanMonitor, {}\n",
    "\n",
    "    predictions = []\n",
    "    samples_ids = []\n",
    "    accuracies = []\n",
    "\n",
    "    tq = tqdm(loader, desc='{} E{:03d}'.format(split, epoch), ncols=0)\n",
    "    loss_tracker = tracker.track('{}_loss'.format(split), tracker_class(**tracker_params))\n",
    "    acc_tracker = tracker.track('{}_acc'.format(split), tracker_class(**tracker_params))\n",
    "    log_softmax = nn.LogSoftmax(dim=1).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in tq:\n",
    "            v = item['visual']\n",
    "            q = item['question']\n",
    "            a = item['answer']\n",
    "            sample_id = item['sample_id']\n",
    "            q_length = item['q_length']\n",
    "\n",
    "            v = Variable(v.cuda())\n",
    "            q = Variable(q.cuda())\n",
    "            a = Variable(a.cuda())\n",
    "            q_length = Variable(q_length.cuda())\n",
    "\n",
    "            out = model(v, q, q_length)\n",
    "\n",
    "            # This is the Soft-loss described in https://arxiv.org/pdf/1708.00584.pdf\n",
    "\n",
    "            nll = -log_softmax(out)\n",
    "\n",
    "            loss = (nll * a / 10).sum(dim=1).mean()\n",
    "            acc = vqa_accuracy(out.data, a.data).cpu()\n",
    "\n",
    "            # save predictions of this batch\n",
    "            _, answer = out.data.cpu().max(dim=1)\n",
    "\n",
    "            predictions.append(answer.view(-1))\n",
    "            accuracies.append(acc.view(-1))\n",
    "            # Sample id is necessary to obtain the mapping sample-prediction\n",
    "            samples_ids.append(sample_id.view(-1).clone())\n",
    "\n",
    "            loss_tracker.append(loss.item())\n",
    "            acc_tracker.append(acc.mean())\n",
    "            fmt = '{:.4f}'.format\n",
    "            tq.set_postfix(loss=fmt(loss_tracker.mean.value), acc=fmt(acc_tracker.mean.value))\n",
    "\n",
    "        predictions = list(torch.cat(predictions, dim=0))\n",
    "        accuracies = list(torch.cat(accuracies, dim=0))\n",
    "        samples_ids = list(torch.cat(samples_ids, dim=0))\n",
    "\n",
    "    eval_results = {\n",
    "        'answers': predictions,\n",
    "        'accuracies': accuracies,\n",
    "        'samples_ids': samples_ids,\n",
    "        'avg_accuracy': acc_tracker.mean.value,\n",
    "        'avg_loss': loss_tracker.mean.value\n",
    "    }\n",
    "\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loader\n",
    "train_loader = get_loader(\"train\",\"mobile_net_train.h5\")\n",
    "val_loader = get_loader(\"val\",\"mobile_net_val.h5\")\n",
    "\n",
    "#Model\n",
    "model = Model(train_loader.dataset.num_tokens)\n",
    "model = model.to(device)\n",
    "\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=0.001)\n",
    "\n",
    "#Tracker\n",
    "tracker = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 10\n",
    "max_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs will be saved in logs\n"
     ]
    }
   ],
   "source": [
    "path_log_dir = \"logs\"\n",
    "\n",
    "if not os.path.exists(path_log_dir):\n",
    "    os.makedirs(path_log_dir)\n",
    "\n",
    "print('Model logs will be saved in {}'.format(path_log_dir))\n",
    "\n",
    "path_best_accuracy = os.path.join(path_log_dir, 'best_accuracy_log.pth')\n",
    "path_best_loss = os.path.join(path_log_dir, 'best_loss_log.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "train E000:   0% 0/156 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "h5py objects cannot be pickled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-5b8630e6b200>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# If we are training on the train split (and not on train+val) we can evaluate on val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-025e2bb2ded5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, tracker, epoch, split)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlog_softmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'visual'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\achaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1106\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1108\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1109\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m             \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\achaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\achaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    717\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\achaa\\appdata\\local\\programs\\python\\python37\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\achaa\\appdata\\local\\programs\\python\\python37\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\achaa\\appdata\\local\\programs\\python\\python37\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\achaa\\appdata\\local\\programs\\python\\python37\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\achaa\\appdata\\local\\programs\\python\\python37\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\achaa\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\h5py\\_hl\\base.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[0mlimitations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlook\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mh5pickle\u001b[0m \u001b[0mproject\u001b[0m \u001b[0mon\u001b[0m \u001b[0mPyPI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \"\"\"\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"h5py objects cannot be pickled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: h5py objects cannot be pickled"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "for i in range(10):\n",
    "    train(model, train_loader, optimizer, tracker, epoch=i, split='train')\n",
    "    # If we are training on the train split (and not on train+val) we can evaluate on val\n",
    "    split='train'\n",
    "    if split == 'train':\n",
    "        eval_results = evaluate(model, val_loader, tracker, epoch=i, split='val')\n",
    "\n",
    "        # save all the information in the log file\n",
    "        log_data = {\n",
    "            'epoch': i,\n",
    "            'tracker': tracker.to_dict(),\n",
    "            'weights': model.state_dict(),\n",
    "            'eval_results': eval_results,\n",
    "            'vocabs': train_loader.dataset.vocabs,\n",
    "        }\n",
    "\n",
    "        # save logs for min validation loss and max validation accuracy\n",
    "        if eval_results['avg_loss'] < min_loss:\n",
    "            torch.save(log_data, path_best_loss)  # save model\n",
    "            min_loss = eval_results['avg_loss']  # update min loss value\n",
    "\n",
    "        if eval_results['avg_accuracy'] > max_accuracy:\n",
    "            torch.save(log_data, path_best_accuracy)  # save model\n",
    "            max_accuracy = eval_results['avg_accuracy']  # update max accuracy value\n",
    "\n",
    "# Save final model\n",
    "log_data = {\n",
    "    'tracker': tracker.to_dict(),\n",
    "    'weights': model.state_dict(),\n",
    "    'vocabs': train_loader.dataset.vocabs,\n",
    "}\n",
    "\n",
    "path_final_log = os.path.join(path_log_dir, 'final_log.pth')\n",
    "torch.save(log_data, path_final_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
